import os
import json
from bs4 import BeautifulSoup
import re

# =========================
#  FONCTIONS UTILITAIRES
# =========================

def clean_text(s):
    if not s:
        return ""
    return " ".join(s.split()).strip()

def extract_item_detail(soup, label):
    for c in soup.select(".tuto-items-details-container"):
        left = c.select_one(".tuto-items-details-container-left")
        right = c.select_one(".tuto-items-details-container-right")
        if not left or not right:
            continue
        if label.lower() in left.get_text(strip=True).lower():
            text = clean_text(right.get_text(" ", strip=True))
            # Nettoyer les doublons anglais/fran√ßais (ex: "Medium Moyen" -> "Moyen")
            # On garde uniquement la partie fran√ßaise (apr√®s le dernier espace si doublon)
            if text and " " in text:
                parts = text.split()
                # Si on a 2 mots et que le premier est en anglais commun, on garde le second
                if len(parts) == 2:
                    first, second = parts
                    # Mapping des valeurs anglaises courantes vers fran√ßais
                    if first.lower() in ("easy", "medium", "hard", "difficult", "very"):
                        return second
                    if first.lower() == "very" and len(parts) > 2:
                        # Cas "Very difficult Tr√®s difficile" -> garder "Tr√®s difficile"
                        return " ".join(parts[2:]) if len(parts) > 2 else parts[-1]
            return text
    return None

def extract_intro(soup):
    intro_div = soup.select_one("div.wf-intro")
    if not intro_div:
        return []
    sections = []

    for elem in intro_div.children:
        if elem.name == "p":
            txt = clean_text(elem.get_text())
            if txt:
                sections.append({"titre": txt, "sous_items": []})

        elif elem.name == "ul":
            items = [clean_text(li.get_text()) for li in elem.find_all("li")]
            items = [i for i in items if i]
            if items:
                if sections:
                    sections[-1]["sous_items"].extend(items)
                else:
                    sections.append({"titre": "", "sous_items": items})
    return sections

def extract_materiaux_outils(soup):
    materiaux = []
    outils = []

    # --- Mat√©riaux ---
    # Sur le wiki, "Mat√©riaux" est souvent dans <span id="Mat√©riaux"> √† l'int√©rieur d'un <h3>
    mat_anchor = soup.find(id="Mat√©riaux")
    if mat_anchor:
        mat_h = mat_anchor.find_parent(["h2", "h3"]) or mat_anchor
        sib = mat_h.next_sibling
        while sib:
            name = getattr(sib, "name", None)
            # On s'arr√™te au prochain titre de section
            if name in ("h2", "h3"):
                break
            # Les mat√©riaux sont list√©s en <p>
            if name == "p":
                txt = clean_text(sib.get_text())
                if txt and txt != "Mat√©riaux":
                    materiaux.append({"titre": txt, "sous_items": []})
            # On r√©cup√®re les <ul> ou <ol> qui contiennent les √©l√©ments de la liste
            elif name in ("ul", "ol"):
                items = [clean_text(li.get_text()) for li in sib.find_all("li")]
                items = [i for i in items if i]
                if items:
                    if materiaux:  # Si on a d√©j√† une entr√©e pour les mat√©riaux
                        materiaux[-1]["sous_items"].extend(items)
                    else:  # Si on n'a pas encore d'entr√©e, on la cr√©e
                        materiaux.append({"titre": "", "sous_items": items})
            sib = sib.next_sibling

    # --- Outils ---
    out_anchor = soup.find(id="Outils")
    if out_anchor:
        out_h = out_anchor.find_parent(["h2", "h3"]) or out_anchor
        sib = out_h.next_sibling
        current_group = None

        while sib:
            name = getattr(sib, "name", None)
            if name in ("h2", "h3"):
                break

            # Certains titres de groupe sont dans <b> tout seul (√âlectricit√©, Autres, EPI)
            if name == "b":
                titre = clean_text(sib.get_text())
                if titre:
                    current_group = {"titre": titre, "sous_items": []}
                    outils.append(current_group)

            elif name == "p":
                txt = clean_text(sib.get_text())
                if not txt:
                    pass
                else:
                    # Cas o√π le <p> contient juste un <b> (titre de groupe)
                    b = sib.find("b")
                    if b and clean_text(b.get_text()) == txt:
                        current_group = {"titre": txt, "sous_items": []}
                        outils.append(current_group)
                    else:
                        # Cas d'un outil normal
                        if current_group is None:
                            current_group = {"titre": "", "sous_items": []}
                            outils.append(current_group)
                        current_group["sous_items"].append(txt)

            # Cas o√π des listes sont pr√©sentes
            elif name in ("ul", "ol"):
                items = [clean_text(li.get_text()) for li in sib.find_all("li")]
                items = [i for i in items if i]
                if items:
                    if current_group is None:
                        current_group = {"titre": "", "sous_items": []}
                        outils.append(current_group)
                    current_group["sous_items"].extend(items)

            sib = sib.next_sibling

    return materiaux, outils



def extract_step_images(step_index, soup):
    carousel_id = f"myCarousel{step_index}"
    car = soup.find(id=carousel_id)
    images = []
    if not car:
        return images

    for img in car.select("img"):
        url = img.get("src")
        if not url:
            continue
        images.append({
            "url": url,
            "alt": clean_text(img.get("alt", "")),
            "description": ""
        })
    return images


def extract_etapes(soup):
    etapes = []

    # Les √©tapes sont dans des <span id="√âtape_..."> √† l'int√©rieur de <h2>
    for anchor in soup.find_all("span", id=re.compile(r"^√âtape_")):
        titre = clean_text(anchor.get_text())
        if not titre:
            continue

        h2 = anchor.find_parent("h2")
        if not h2:
            continue

        # Num√©ro d'√©tape : "√âtape 3 - ..." -> 3
        m = re.search(r"√âtape\s*(\d+)", titre)
        numero = int(m.group(1)) if m else len(etapes) + 1

        etapes_list = []
        remarques = []
        images = []

        # Parcours de TOUT ce qui suit h2 jusqu'au prochain h2
        for node in h2.next_elements:
            # On s'arr√™te au prochain h2 (mais pas le h2 courant)
            if getattr(node, "name", None) == "h2" and node is not h2:
                break

            if not hasattr(node, "name"):
                continue

            # Texte des √©tapes : paragraphes
            if node.name == "p":
                txt = clean_text(node.get_text())
                if txt:
                    etapes_list.append({"titre": txt, "sous_etapes": []})

            # Listes ordonn√©es ou non ordonn√©es -> sous-√©tapes
            elif node.name in ("ul", "ol"):
                items = [clean_text(li.get_text()) for li in node.find_all("li")]
                items = [i for i in items if i]
                if items:
                    etapes_list.append({"titre": "", "sous_etapes": items})

            # IMAGES (y compris dans .tuto-step-image, car on parcourt tout)
            elif node.name == "img":
                url = node.get("src")
                if not url:
                    continue
                alt = clean_text(node.get("alt") or "")
                images.append({
                    "url": url,
                    "alt": alt,
                    "description": ""
                })

        # Nettoyage : si aucune √©tape textuelle n‚Äôa √©t√© trouv√©e, on ignore
        if not etapes_list and not images:
            continue

        solution = {
            "objectif": "",
            "materiaux_outils": [],
            "etapes": etapes_list,
            "remarques": remarques,
            "fichiers": [{
                "nom": "",
                "type": "",
                "description": "",
                "lien": ""
            }],
            "images": images
        }

        etapes.append({
            "numero": numero,
            "titre": titre,
            "solutions": [solution]
        })

    return etapes




# =========================
#   CONVERT HTML ‚Üí JSON
# =========================

def html_to_makerlens_json(html: str, page_url: str, slug: str):
    soup = BeautifulSoup(html, "html.parser")

    h1 = soup.find("h1", id="firstHeading")
    titre_page = clean_text(h1.get_text()) if h1 else slug

    main_img = soup.select_one(".tuto-main-image img")
    images = []
    if main_img:
        images.append({
            "url": main_img.get("src"),
            "alt": clean_text(main_img.get("alt")),
            "description": clean_text(
                soup.select_one(".tuto-details-about-title").get_text()
                if soup.select_one(".tuto-details-about-title") else ""
            )
        })

    difficulte = extract_item_detail(soup, "Difficult√©")
    duree = extract_item_detail(soup, "Dur√©e")
    cout = extract_item_detail(soup, "Co√ªt")

    intro = extract_intro(soup)
    materiaux, outils = extract_materiaux_outils(soup)
    etapes = extract_etapes(soup)

    return {
        slug: {
            "titre": [titre_page],
            "image": images,
            "difficult√©": [difficulte] if difficulte else [],
            "dur√©e": [duree] if duree else [],
            "co√ªt": [cout] if cout else [],
            "liens": [page_url],
            "introduction": intro,
            "mentions l√©gales": [],
            "mat√©riaux": materiaux,
            "outils": outils,
            "√©tapes": etapes,
            "annexes": []
        }
    }


# =========================
#   BATCH LOWTECHLAB
# =========================

def batch_convert_lowtechlab():
    # dossier du fichier actuel (analyseurs/)
    here = os.path.dirname(os.path.abspath(__file__))

    # racine du projet (scripts_recuperation/)
    project_root = os.path.dirname(here)

    # dossiers HTML et JSON
    pages_dir = os.path.join(project_root, "pages_brutes", "lowtechlab")
    output_dir = os.path.join(project_root, "donnees_brutes", "lowtechlab")
    os.makedirs(output_dir, exist_ok=True)

    base_url = "https://wiki.lowtechlab.org/wiki/"

    # lister tous les fichiers HTML
    for filename in os.listdir(pages_dir):
        if not filename.lower().endswith(".html"):
            continue

        html_path = os.path.join(pages_dir, filename)
        # exemple: "V√©lo_g√©n√©rateur_d_√©lectricit√©.html" -> "V√©lo_g√©n√©rateur_d_√©lectricit√©"
        base_name = os.path.splitext(filename)[0]

        # cl√© du JSON : version "humaine" (underscore -> espace)
        slug = base_name.replace("_", " ")

        # URL d'origine probable
        page_url = base_url + base_name

        # fichier de sortie JSON (on garde le m√™me base_name)
        out_path = os.path.join(output_dir, base_name + ".json")

        print(f"üîß Traitement de : {html_path}")

        with open(html_path, "r", encoding="utf-8") as f:
            html = f.read()

        data = html_to_makerlens_json(html, page_url, slug)

        with open(out_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=4)

        print(f"‚úÖ JSON g√©n√©r√© : {out_path}")

    print("üéâ Batch Low-tech Lab termin√©.")


# =========================
#     MAIN
# =========================

if __name__ == "__main__":
    batch_convert_lowtechlab()
