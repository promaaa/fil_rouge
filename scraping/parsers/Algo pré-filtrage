import os
import re
from bs4 import BeautifulSoup

# --- FONCTION 1 : segmentation simple par titres ---
def segment_by_headings(soup):
    sections = []
    current = {"title": "Introduction", "content": []}
    for elem in soup.find_all(["h1", "h2", "h3", "p", "ul", "ol", "img"]):
        if elem.name in ("h1", "h2", "h3"):
            sections.append(current)
            current = {"title": elem.get_text(" ", strip=True), "content": []}
        else:
            current["content"].append(elem)
    sections.append(current)
    return sections

# --- FONCTION 2 : nettoyage + segmentation + labellisation ---
def clean_and_segment(html):
    soup = BeautifulSoup(html, "html.parser")

    # Nettoyage de base (menus, scripts, etc.)
    for tag in soup(["script", "style", "nav", "footer", "header", "iframe", "svg"]):
        tag.decompose()

    # Segmentation en sections
    sections = []
    current = {"title": "Introduction", "blocks": []}
    for elem in soup.find_all(["h1","h2","h3","p","ul","ol","img"]):
        if elem.name in ("h1","h2","h3"):
            sections.append(current)
            current = {"title": elem.get_text(" ", strip=True), "blocks": []}
        else:
            current["blocks"].append(elem)
    sections.append(current)

    # Labellisation heuristique selon les titres
    for s in sections:
        title = s["title"].lower()
        if any(k in title for k in ["matériaux","materials","supplies"]):
            s["label"] = "MATERIAUX"
        elif any(k in title for k in ["outils","tools","equipment"]):
            s["label"] = "OUTILS"
        elif any(k in title for k in ["étape","step"]):
            s["label"] = "ETAPES"
        elif any(k in title for k in ["mentions","license","licence"]):
            s["label"] = "MENTIONS"
        else:
            s["label"] = "INTRO"
    return sections

# --- FONCTION 3 (optionnelle) : évaluation densité texte ---
def score_section(section):
    text = " ".join(b.get_text(" ", strip=True) for b in section.get("blocks", []))
    n_chars = len(text)
    n_imgs = sum(1 for b in section.get("blocks", []) if b.name == "img")
    density = n_chars / max(len(str(section)), 1)
    score = n_chars + 50 * n_imgs + 200 * density
    return score



# --- PIPELINE PRINCIPAL (lowtechlab) ---

in_dir  = "scraping/pages_html/lowtechlab"
out_dir = "pages_clean/lowtechlab"
os.makedirs(out_dir, exist_ok=True)

for filename in os.listdir(in_dir):
    if not filename.endswith(".html"):
        continue

    in_path  = os.path.join(in_dir, filename)
    out_path = os.path.join(out_dir, filename)
    with open(in_path, "r", encoding="utf-8") as f:
        html = f.read()

    soup = BeautifulSoup(html, "html.parser")

    # A) Nettoyage de base
    soup = BeautifulSoup(html, "html.parser")
    for tag in soup(["script", "style", "nav", "footer", "header", "iframe", "svg"]):
        tag.decompose()

    with open(out_path, "w", encoding="utf-8") as f:
        f.write(str(soup))  # ou soup.prettify()

    print(f"✅ 'Débruité' : {filename} ({len(soup.text)} caractères de texte)")

    # B) Segmentation + labellisation
    sections = clean_and_segment(str(soup))
    print(f"→ {len(sections)} sections détectées dans {filename}")

    # C) (Optionnel) : afficher aperçu des titres et labels
    # for i, s in enumerate(sections, 1):
    #     title = s["title"]
    #     label = s.get("label", "?")
    #     print(f"   {i:02d}. [{label}] {title}")



# --- PIPELINE PRINCIPAL (wikifab) ---

in_dir  = "scraping/pages_html/wikifab"
out_dir = "pages_clean/wikifab"
os.makedirs(out_dir, exist_ok=True)

for filename in os.listdir(in_dir):
    if not filename.endswith(".html"):
        continue

    in_path  = os.path.join(in_dir, filename)
    out_path = os.path.join(out_dir, filename)
    with open(in_path, "r", encoding="utf-8") as f:
        html = f.read()

    soup = BeautifulSoup(html, "html.parser")

    # A) Nettoyage de base
    soup = BeautifulSoup(html, "html.parser")
    for tag in soup(["script", "style", "nav", "footer", "header", "iframe", "svg"]):
        tag.decompose()

    with open(out_path, "w", encoding="utf-8") as f:
        f.write(str(soup))  # ou soup.prettify()

    print(f"'Débruité' : {filename} ({len(soup.text)} caractères de texte)")

    # B) Segmentation + labellisation
    sections = clean_and_segment(str(soup))
    print(f"→ {len(sections)} sections détectées dans {filename}")

    # C) (Optionnel) : afficher aperçu des titres et labels
    # for i, s in enumerate(sections, 1):
    #     title = s["title"]
    #     label = s.get("label", "?")
    #     print(f"   {i:02d}. [{label}] {title}")



# --- PIPELINE PRINCIPAL (pour sites instructables) ---

in_dir  = "scraping/pages_html/instructables"
out_dir = "pages_clean/instructables"
os.makedirs(out_dir, exist_ok=True)

for filename in os.listdir(in_dir):
    if not filename.endswith(".html"):
        continue

    in_path  = os.path.join(in_dir, filename)
    out_path = os.path.join(out_dir, filename)
    with open(in_path, "r", encoding="utf-8") as f:
        html = f.read()

    soup = BeautifulSoup(html, "html.parser")

    # A) Nettoyage de base
    soup = BeautifulSoup(html, "html.parser")
    for tag in soup(["script", "style", "nav", "footer", "header", "iframe", "svg"]):
        tag.decompose()

    with open(out_path, "w", encoding="utf-8") as f:
        f.write(str(soup))  # ou soup.prettify()

    print(f"✅ 'Débruité' : {filename} ({len(soup.text)} caractères de texte)")

    # B) Segmentation + labellisation
    sections = clean_and_segment(str(soup))
    print(f"→ {len(sections)} sections détectées dans {filename}")

    # C) (Optionnel) : afficher aperçu des titres et labels
    # for i, s in enumerate(sections, 1):
    #     title = s["title"]
    #     label = s.get("label", "?")
    #     print(f"   {i:02d}. [{label}] {title}")
